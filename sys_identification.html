
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>sys_identification</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-08-21"><meta name="DC.source" content="sys_identification.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">System Identification for Echo Cancellation</a></li><li><a href="#2">Pure LMS</a></li><li><a href="#3">Sign LMS</a></li><li><a href="#4">Sign-Regressor LMS</a></li><li><a href="#5">Signed-Signed LMS</a></li><li><a href="#6">Normalized LMS</a></li><li><a href="#7">Affine Projection LMS</a></li><li><a href="#8">Recursive Least Squares</a></li><li><a href="#9">Frequency Response Comparision</a></li><li><a href="#10">Computation Time Comparision</a></li><li><a href="#11">Comparision and Discussion of all the Algorithms</a></li></ul></div><h2 id="1">System Identification for Echo Cancellation</h2><p>By: Rohan Rohilla</p><pre class="codeinput">clear
close <span class="string">all</span>
clc
</pre><h2 id="2">Pure LMS</h2><p>Discussion: LMS is very simple and stable algorithm. But its convergence rate depends on the eigenvalues of the autocorrelation of the input and thus make it less usable in practical situations (in other words LMS is sensitive to the scaling of its input). From the simulations it is observed that the oerder of the filter is inversely proportion to the convergece rate. To maintain the stability of the algorithm mu has to be lower than 1/(3*tr[R]).</p><pre class="codeinput">tic
comp_times = zeros(1,7);
tim = cputime;
H = [0.35 1 -0.35]';
<span class="comment">% H = [0.35 1 0.35]';</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
N = 15;<span class="comment">%6,10,15 (Order of the Filter)</span>
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.2;
mu = misad/trace(R);
in_p_samps = 400; <span class="comment">% number of samples</span>
var = 0.001;
realizations = 600;
w1 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
<span class="keyword">for</span> m = 1:realizations <span class="comment">% Number of realizations</span>
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    <span class="keyword">for</span> n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w1(:,n)'*X);
        e(n) = d(n) - y(n);
        w1(:,n+1) = w1(:,n) + 2*mu*e(n)*X; <span class="comment">% Recursion Equation</span>
    <span class="keyword">end</span>
    SE(m,:) = e.^2; <span class="comment">% Square Error</span>
<span class="keyword">end</span>
comp_times(1) = cputime - tim;
t = 1:in_p_samps; <span class="comment">% Mean Square Error</span>
MMSE  = var*ones(in_p_samps,1); <span class="comment">% Minimum MSE</span>
MSE = mean(SE);
figure;
semilogy(t,MSE,t,MMSE,<span class="string">'--k'</span>)
title([<span class="string">'Pure LMS for N = '</span>, num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'MSE'</span>)
legend(<span class="string">'MSE Convergence'</span>,<span class="string">'Minimum MSE'</span>);
grid

figure;
plot(t,w1(:,1:end-1))
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'Coefficients'</span>)
title(<span class="string">'Pure LMS Coefficient Convergence'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_01.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_02.png" alt=""> <h2 id="3">Sign LMS</h2><p>Discussion: Sign LMS was developed to reduce the computational complexity of the pure LMS on the cost of convergence speed and this can be noted easily from the Bar graph at the end of the program. Sign LMS takes around 1200 iterations to converge to the optimal coefficients. This is because we can see the step-size parameter as a function of the error (mu' = mu/|e(n)|) Here initially step size is very small because of the huge <tt>e(n)</tt> but as the error decreases the step-size increases and this can be seen in the plot. For Sign LMS as well convergence rate decreasaes when we increase the order of the filter.</p><pre class="codeinput">clearvars <span class="string">-except</span> <span class="string">H</span> <span class="string">w0_den</span> <span class="string">w0_num</span> <span class="string">w1</span> <span class="string">comp_times</span>
H = [0.35 1 -0.35]';
<span class="comment">% H = [0.35 1 0.35]';</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;<span class="comment">%6,10,15 (Order of the Filter)</span>
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.010;
mu = misad/trace(R);
in_p_samps = 2000; <span class="comment">% number of samples</span>
var = 0.001;
realizations = 2000;
w2 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
<span class="keyword">for</span> m = 1:realizations <span class="comment">% Number of realizations</span>
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    <span class="keyword">for</span> n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w2(:,n)'*X);
        e(n) = d(n) - y(n);
        mu_dash = mu/abs(e(n));
        w2(:,n+1) = w2(:,n) + 2*mu_dash.*(e(n))*X; <span class="comment">% Recursion Equation</span>
    <span class="keyword">end</span>
    SE(m,:) = (e).^2; <span class="comment">% Square Error</span>
<span class="keyword">end</span>
comp_times(2) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); <span class="comment">% Mean Square Error</span>
MMSE  = var*ones(in_p_samps,1); <span class="comment">% Minimum MSE</span>
figure;
semilogy(t,MSE,t,MMSE,<span class="string">'--k'</span>)
title([<span class="string">'Sign LMS for N = '</span>, num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'MSE'</span>)
legend(<span class="string">'MSE Convergence'</span>,<span class="string">'Minimum MSE'</span>);
grid

figure;
plot(t,w2(:,1:end-1))
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'Coefficients'</span>)
title(<span class="string">'Signed LMS Coefficient Convergence'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_03.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_04.png" alt=""> <h2 id="4">Sign-Regressor LMS</h2><p>Discussion: Although built on the same line as sign LMS (to reduce the computational complexity) Sign regressor is almost as fast as pure LMS because in this case the new step size doesn't depend on the filter convergence. The step size is chosen on the basis of the average of <tt>x(n)</tt> and this gives a more homogeneous convergence. Also on a similar oberservation about the learning curve and the order of the filter can be made.</p><pre class="codeinput">clearvars <span class="string">-except</span> <span class="string">H</span> <span class="string">w0_den</span> <span class="string">w0_num</span> <span class="string">w1</span> <span class="string">w2</span> <span class="string">comp_times</span>
H = [0.35 1 -0.35]';
<span class="comment">% H = [0.35 1 0.35]';</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;<span class="comment">%6,10,15 (Order of the Filter)</span>
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.2;
mu = 0.0107; <span class="comment">%misad/trace(R)</span>
in_p_samps = 400; <span class="comment">% number of samples</span>
var = 0.001;
realizations = 1000;
w3 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
<span class="keyword">for</span> m = 1:realizations <span class="comment">% Number of realizations</span>
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    <span class="keyword">for</span> n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w3(:,n)'*X);
        e(n) = d(n) - y(n);
        w3(:,n+1) = w3(:,n) + 2*mu*e(n)*X./abs(X); <span class="comment">% Recursion Equation</span>
    <span class="keyword">end</span>
    SE(m,:) = (e).^2; <span class="comment">% Square Error</span>
<span class="keyword">end</span>
comp_times(3) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); <span class="comment">% Mean Square Error</span>
MMSE  = var*ones(in_p_samps,1); <span class="comment">% Minimum MSE</span>
figure;
semilogy(t,MSE,t,MMSE,<span class="string">'--k'</span>)
title([<span class="string">'Sign Regressor LMSfor N = '</span>, num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'MSE'</span>)
legend(<span class="string">'MSE Convergence'</span>,<span class="string">'Minimum MSE'</span>);
grid

figure;
plot(t,w3(:,1:end-1))
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'Coefficients'</span>)
title(<span class="string">'Sign Regressor Coefficient Convergence'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_05.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_06.png" alt=""> <h2 id="5">Signed-Signed LMS</h2><p>Discussion: Sign-sign LMS is nothing but the mixture of sign and sign regressor algorithm. Among given two algorithms it is close to sign because its step size parameter depends on the filter convergence and therefore shows the same characteristics as the sign algorithm. Because it doesn't use the values of the error and input it is computationally simple and can be used in practical scenarios where speed is not necessary for example in medical equipment.</p><pre class="codeinput">clearvars <span class="string">-except</span> <span class="string">H</span> <span class="string">w0_den</span> <span class="string">w0_num</span> <span class="string">w1</span> <span class="string">w2</span> <span class="string">w3</span> <span class="string">comp_times</span>
H = [0.35 1 -0.35]';
<span class="comment">% H = [0.35 1 0.35]';</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;<span class="comment">%6,10,15 (Order of the Filter)</span>
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.2;
mu = 0.0003; <span class="comment">%misad/trace(R) (Step Size)</span>
in_p_samps = 4000; <span class="comment">% number of samples</span>
var = 0.001;
realizations = 600;
w4 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
<span class="keyword">for</span> m = 1:realizations  <span class="comment">% Number of realizations</span>
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    <span class="keyword">for</span> n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w4(:,n)'*X);
        e(n) = d(n) - y(n);
        w4(:,n+1) = w4(:,n) + 2*mu.*sign(e(n)*X); <span class="comment">% Recursion Equation</span>
    <span class="keyword">end</span>
    SE(m,:) = e.^2; <span class="comment">% Square Error</span>
<span class="keyword">end</span>
comp_times(4) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); <span class="comment">% Mean Square Error</span>
MMSE  = var*ones(in_p_samps,1); <span class="comment">% Minimum MSE</span>
figure;
semilogy(t,MSE,t,MMSE,<span class="string">'--k'</span>)
title([<span class="string">'Signed-Signed LMS for N = '</span>, num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'MSE'</span>)
legend(<span class="string">'MSE Convergence'</span>,<span class="string">'Minimum MSE'</span>);
grid

figure;
plot(t,w4(:,1:end-1))
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'Coefficients'</span>)
title(<span class="string">'Signed Signed LMS Coefficient Convergence'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_07.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_08.png" alt=""> <h2 id="6">Normalized LMS</h2><p>Discussion: Major Problem with Pure LMS algorithm is that its sensitivity depends on the scaling of the input. Normalized LMS takes care of this problem by normalizing the power of the input. Here mu^ and Psi are positive constants, mu^ can be thought of as a step size parameter which controls the convergence of the algorithm and Psi is added in the denominator in order to prevent a division by a small number (x'x -&gt; Euclidean Norm). This results in a stable and a fast converging adaptation algorithm. When the value for mu^ is changed the convergence behavior changes and by that it can be said that the mu^ is directly proportional to the convergence rate (if we change mu^ from 0.5 to 1 rate of convergence increases).</p><pre class="codeinput">clearvars <span class="string">-except</span> <span class="string">H</span> <span class="string">w0_den</span> <span class="string">w0_num</span> <span class="string">w1</span> <span class="string">w2</span> <span class="string">w3</span> <span class="string">w4</span> <span class="string">comp_times</span>
H = [0.35 1 -0.35]';
<span class="comment">% H = [0.35 1 0.35]';</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;<span class="comment">%6,10,15 (Order of the Filter)</span>
mu_hat = 1;<span class="comment">%1,0.5</span>
sci = 0.0001;
in_p_samps = 400; <span class="comment">% number of samples</span>
var = 0.001;
realizations = 600;
w5 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
<span class="keyword">for</span> m = 1:realizations  <span class="comment">% Number of realizations</span>
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    <span class="keyword">for</span> n = N:in_p_samps <span class="comment">% number of runs</span>
        X = x(n:-1:n-N+1);
        y(n) = (w5(:,n)'*X);
        e(n) = d(n) - y(n);
        w5(:,n+1) = w5(:,n) + mu_hat*(e(n)*X)./(2*(X'*X) + sci); <span class="comment">% Recursion Equation</span>
    <span class="keyword">end</span>
    SE(m,:) = e.^2; <span class="comment">% Square Error</span>
<span class="keyword">end</span>
comp_times(5) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); <span class="comment">% Mean Square Error</span>
MMSE  = var*ones(in_p_samps,1); <span class="comment">% Minimum MSE</span>
figure;
semilogy(t,MSE,t,MMSE,<span class="string">'--k'</span>)
title([<span class="string">'Normalized LMS with Mu Hat = '</span>, num2str(mu_hat),<span class="string">' and N = '</span>,num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'MSE'</span>)
legend(<span class="string">'MSE Convergence'</span>,<span class="string">'Minimum MSE'</span>);
grid

figure;
plot(t,w5(:,1:end-1))
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'Coefficients'</span>)
title(<span class="string">'NLMS Coefficient Convergence'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_09.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_10.png" alt=""> <h2 id="7">Affine Projection LMS</h2><p>Discussion: Affine Projection is a generalized form or Normalized LMS which can also be called as the Orthogonal Projection LMS. Here mu^ and Psi are used for the same purposes as in the NLMS algorithm. From the simulations it can be seen that the convergence rate increases as we increase M. For M = 1 it behaves like Normalized LMS. Computation time of APLMS for the simulation is very high as well (Just like its computation complexity).</p><pre class="codeinput">clearvars <span class="string">-except</span> <span class="string">H</span> <span class="string">w0_den</span> <span class="string">w0_num</span> <span class="string">w1</span> <span class="string">w2</span> <span class="string">w3</span> <span class="string">w4</span> <span class="string">w5</span> <span class="string">comp_times</span>
H = [0.35 1 -0.35]';
<span class="comment">% H = [0.35 1 0.35]';</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;<span class="comment">%10,6,15 (Order of the Filter)</span>
mu_hat = 0.5;<span class="comment">%1</span>
psi = 1e-4;
M = 2;<span class="comment">%2,4,6,8 % number of colums</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
in_p_samps = 800;
var = 0.001;
realizations = 600;
<span class="keyword">for</span> p = 1:realizations
    v = randn(1,in_p_samps)';
    x = filter(H,1,v);<span class="comment">% Generation of input x(n)</span>
    d = filter(w0_num,w0_den,x)+sqrt(var)*randn(in_p_samps,1);
    XAF = zeros(N,M);
    w6 = zeros(N,in_p_samps);
    <span class="keyword">for</span> m = N+M:in_p_samps
        <span class="keyword">for</span> k = 1:M
            XAF(:,k) = x(m-k+1:-1:m-k+1-N+1);
        <span class="keyword">end</span>
        E = d(m:-1:m-M+1) - XAF'*w6(:,m);
        w6(:,m+1) = w6(:,m) + mu_hat*XAF*inv((XAF'*XAF + psi*eye(M)))*E;
        e(m) = E(1)'*E(1);
    <span class="keyword">end</span>
    SE(p,:) = e';
<span class="keyword">end</span>
comp_times(6) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); <span class="comment">% Mean Square Error</span>
MMSE  = var*ones(in_p_samps,1); <span class="comment">% Minimum MSE</span>
figure;
semilogy(t,MSE,t,MMSE,<span class="string">'--k'</span>)
title([<span class="string">'Affine Projection LMS with Mu^ = '</span>, num2str(mu_hat),<span class="string">' and M = '</span> num2str(M)])
axis([0,in_p_samps,var*0.8,100]);
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'MSE'</span>)
legend(<span class="string">'MSE Convergence'</span>,<span class="string">'Minimum MSE'</span>);
grid

figure;
plot(t,w6(:,1:end-1))
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'Coefficients'</span>)
title(<span class="string">'APLMS Coefficient Convergence'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_11.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_12.png" alt=""> <h2 id="8">Recursive Least Squares</h2><p>Discussion: RLS is different from the rest of the algorithms because unlike others it is not based on stochastic behavior of the processes. It is assumed that all the processes are ergodic and thus their time average is taken instead of the ensemble average. Thus RLS is a deterministic algorithm. This property of RLS makes it more usable in real situations. It is observed from the simulations that the computation time of RLS algorithm is better than APLMS and Sign LMS, and it takes the least number of iterations to converge (for a particular set of parameters). From the plots it is also verified that the fast convergence starts right after the N samples of input. This algorithm was tested for different values of delta and no change in the convergence rate and computation time was observed. The variable Lambda is forgetting factor and it governs the convergence of the algorithm. For a lambda as low as 0.1 the algorithm diverges and if the lambda value is increased the MSE decreases. One strange thing I observed in the simulation was that, if I increase the value of Lambda to 0.9 the MSE decreases, this time even lower than the minimum MSE!!</p><pre class="codeinput">clearvars <span class="string">-except</span> <span class="string">H</span> <span class="string">w0_den</span> <span class="string">w0_num</span> <span class="string">w1</span> <span class="string">w2</span> <span class="string">w3</span> <span class="string">w4</span> <span class="string">w5</span> <span class="string">w6</span> <span class="string">comp_times</span>
H = [0.35 1 -0.35]';
<span class="comment">% H = [0.35 1 0.35]';</span>
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;<span class="comment">%10,6,15 (Order of the Filter)</span>
delta = 1e0;<span class="comment">% 1e-3,1e-2,1e-1,1</span>
lambda = 0.8; <span class="comment">% Forgetting Factor</span>
var = 0.001;
in_p_samps = 400;
sd = sqrt(0.001);
realizations = 600;
s = (1/delta)*eye(15);
w7 = zeros(15,(in_p_samps));
<span class="keyword">for</span> n = 1:realizations <span class="comment">% Deifferent Realizations</span>
    v = randn(1,in_p_samps)';
    x = filter(H,1,v);<span class="comment">% Generation of input x(n)</span>
    d = filter(w0_num,w0_den,x) + (sd)*rand(in_p_samps,1);
    u = zeros(N,1);
    s = (1/delta)*eye(15);
    y = zeros(1,in_p_samps);
    e = zeros(1,in_p_samps);
    xaf = zeros(N,1);
    n_temp = zeros(N);
    k = zeros(1,in_p_samps);
    <span class="keyword">for</span> m = N:in_p_samps
        xaf = x(m:-1:m-N+1);
        u = s*xaf;
        k= u./(lambda+(xaf'*u));
        y(m) = w7(:,m)'*xaf;
        e(m) = d(m) - y(m);
        w7(:,m+1) = w7(:,m) + k*e(m); <span class="comment">% Recursion Equation</span>
        <span class="comment">% n_temp = (lambda^-1)*(s - k*xaf'*s); % to maintain symmetry</span>
        <span class="comment">% st = triu(n_temp,1);d_s = eye(15).*diag(n_temp);</span>
        <span class="comment">% s = st + d_s + st'; % Making Symmetric s matrix (Psi Inverse matrix)</span>
        s = (lambda^-1)*(s - k*xaf'*s); <span class="comment">% Comment this</span>
    <span class="keyword">end</span>
    SE(n,:) = e.^2; <span class="comment">% Square Error</span>
<span class="keyword">end</span>
comp_times(7) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); <span class="comment">% Mean Square Error</span>
MMSE  = var*ones(in_p_samps,1); <span class="comment">% Minimum MSE</span>
figure;
semilogy(t,MSE,t,MMSE,<span class="string">'--k'</span>)
title([<span class="string">'RLS with \delta = '</span>, num2str(delta),<span class="string">' and \lambda = '</span> num2str(lambda)])
axis([0,in_p_samps,var*0.8,100]);
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'MSE'</span>)
legend(<span class="string">'MSE Convergence'</span>,<span class="string">'Minimum MSE'</span>);
grid

figure;
plot(t,w7(:,1:end-1))
xlabel(<span class="string">'NO. OF ITERATIONS'</span>)
ylabel(<span class="string">'Coefficients'</span>)
title(<span class="string">'RLS Coefficient Convergence'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_13.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_14.png" alt=""> <h2 id="9">Frequency Response Comparision</h2><pre class="codeinput">lms = w1(:,end); <span class="comment">% Modeled System Coefficients for LMS</span>
slms = w2(:,end); <span class="comment">% Modeled System Coefficients for Sign LMS</span>
srlms = w3(:,end); <span class="comment">% Modeled System Coefficients for Sign Regressor LMS</span>
sslms = w4(:,end); <span class="comment">% Modeled System Coefficients for Signed-Signed LMS</span>
nlms = w5(:,end); <span class="comment">% Modeled System Coefficients for Normalized LMS</span>
aplms = w6(:,end); <span class="comment">% Modeled System Coefficients for Affine Projection LMS</span>
rls = w7(:,end); <span class="comment">% Modeled System Coefficients for Recursive Lear Squares</span>

[LMS,wlms]=freqz(lms,1);
[SLMS,wslms]=freqz(slms,1);
[SRLMS,wsrlms]=freqz(srlms,1);
[SSLMS,wsslms]=freqz(sslms,1);
[NLMS,wnlms]=freqz(nlms,1);
[APLMS,waplms]=freqz(aplms,1);
[RLS,wrls]=freqz(rls,1);

[OR_FILT,wor_filt] = freqz(w0_num,w0_den); <span class="comment">% Frequency Response of the Original IIR filter</span>

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wlms/(2*pi),20*log10(abs(LMS)),<span class="string">'linewidth'</span>,2)
legend(<span class="string">'Original IIR Filter'</span>,<span class="string">'Regular LMS'</span>)
ylabel(<span class="string">'Log_magnitude'</span>);
xlabel(<span class="string">'Normalized Frequency'</span>);
grid <span class="string">on</span>

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wslms/(2*pi),20*log10(abs(SLMS)),<span class="string">'linewidth'</span>,2)
legend(<span class="string">'Original IIR Filter'</span>,<span class="string">'Sign LMS'</span>)
ylabel(<span class="string">'Log_magnitude'</span>);
xlabel(<span class="string">'Normalized Frequency'</span>);
title(<span class="string">'Frequency Response Comparision Between the Original IIR filter and the Identified Filter'</span>)
grid <span class="string">on</span>

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wsrlms/(2*pi),20*log10(abs(SRLMS)),<span class="string">'linewidth'</span>,2)
legend(<span class="string">'Original IIR Filter'</span>,<span class="string">'Sign Regressor LMS'</span>)
ylabel(<span class="string">'Log_magnitude'</span>);
xlabel(<span class="string">'Normalized Frequency'</span>);
title(<span class="string">'Frequency Response Comparision Between the Original IIR filter and the Identified Filter'</span>)
grid <span class="string">on</span>

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wsslms/(2*pi),20*log10(abs(SSLMS)),<span class="string">'linewidth'</span>,2)
legend(<span class="string">'Original IIR Filter'</span>,<span class="string">'Sign Sign LMS'</span>)
ylabel(<span class="string">'Log_magnitude'</span>);
xlabel(<span class="string">'Normalized Frequency'</span>);
title(<span class="string">'Frequency Response Comparision Between the Original IIR filter and the Identified Filter'</span>)
grid <span class="string">on</span>

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wnlms/(2*pi),20*log10(abs(NLMS)),<span class="string">'linewidth'</span>,2)
legend(<span class="string">'Original IIR Filter'</span>,<span class="string">'Normalized LMS'</span>)
ylabel(<span class="string">'Log_magnitude'</span>);
xlabel(<span class="string">'Normalized Frequency'</span>);
title(<span class="string">'Frequency Response Comparision Between the Original IIR filter and the Identified Filter'</span>)
grid <span class="string">on</span>

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),waplms/(2*pi),20*log10(abs(APLMS)),<span class="string">'linewidth'</span>,2)
legend(<span class="string">'Original IIR Filter'</span>,<span class="string">'Affine Projection LMS'</span>)
ylabel(<span class="string">'Log_magnitude'</span>);
xlabel(<span class="string">'Normalized Frequency'</span>);
title(<span class="string">'Frequency Response Comparision Between the Original IIR filter and the Identified Filter'</span>)
grid <span class="string">on</span>

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wrls/(2*pi),20*log10(abs(RLS)),<span class="string">'linewidth'</span>,2)
legend(<span class="string">'Original IIR Filter'</span>,<span class="string">'Regular LMS'</span>)
ylabel(<span class="string">'Log_magnitude'</span>);
xlabel(<span class="string">'Normalized Frequency'</span>);
title(<span class="string">'Frequency Response Comparision Between the Original IIR filter and the Identified Filter'</span>)
grid <span class="string">on</span>
</pre><img vspace="5" hspace="5" src="sys_identification_15.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_16.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_17.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_18.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_19.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_20.png" alt=""> <img vspace="5" hspace="5" src="sys_identification_21.png" alt=""> <h2 id="10">Computation Time Comparision</h2><pre class="codeinput">bar_name = {<span class="string">'LMS'</span>,<span class="string">'SLMS'</span>,<span class="string">'SRLMS'</span>,<span class="string">'SSLMS'</span>,<span class="string">'NLMS'</span>,<span class="string">'APLMS'</span>,<span class="string">'RLS'</span>};
figure;
bar(comp_times)
title(<span class="string">'Convergence Time for each Algorithm (in s)'</span>)
ylabel(<span class="string">'Time in Seconds'</span>)
xlabel(<span class="string">'Algorithms'</span>)
grid
set(gca,<span class="string">'xticklabel'</span>, bar_name)
toc
</pre><pre class="codeoutput">Elapsed time is 54.076545 seconds.
</pre><img vspace="5" hspace="5" src="sys_identification_22.png" alt=""> <h2 id="11">Comparision and Discussion of all the Algorithms</h2><pre class="codeinput"><span class="comment">% At the end we can pick a winner in terms of computation time, computation</span>
<span class="comment">% complexity, number of iterations required (and also on number of</span>
<span class="comment">% realization required, but in practical situations we only have one</span>
<span class="comment">% realization so there is not point of taking number of realizations as a</span>
<span class="comment">% performance metric). In case of computation complexity it is well known</span>
<span class="comment">% that the sign-sign LMS beats all of these algorithms (from the textbook</span>
<span class="comment">% Adaptive Filters theory and application). But on the other hand it takes</span>
<span class="comment">% more iterations than the rest of the algorithms to converge.</span>

<span class="comment">% Similar to sign sign lms, sign lms and sign regressor also takes more</span>
<span class="comment">% than reasonable iterations. In their defense there job was not to reduce</span>
<span class="comment">% the number of iteration but to reduce the computation complexity.</span>

<span class="comment">% For faster convergence I would pick NLMS over any of the give algorithms</span>
<span class="comment">% because it took significantly less time and iterations. Although Pure LMS</span>
<span class="comment">% beats NLMS when it comes to computation time but it takes more iterations</span>
<span class="comment">% than NLMS as well and the difference in timings is not that big. Again</span>
<span class="comment">% there is no free lunch! NLMS is beautiful but it based on statistical</span>
<span class="comment">% parameters and thus requires more realizations than 1 (and that&#8217;s all what</span>
<span class="comment">% we have got in reality). This is where RLS comes into play. It is purely</span>
<span class="comment">% deterministic and speed is comparable as well. With this facility comes</span>
<span class="comment">% the cost and that is more computations (RLS requires 4N^2 multiplications</span>
<span class="comment">% and 3N^2 additions).</span>

<span class="comment">% Frequency responses of the identified filters (from each algorithm) are</span>
<span class="comment">% also plotted and compared with the original IIR filter. All of them</span>
<span class="comment">% matches very well, except that a few have ripple in the stop band and</span>
<span class="comment">% in the pass band. As the Filters are just the approximation to the</span>
<span class="comment">% original filter this kind of side effect is bound to come across.</span>
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% System Identification for Echo Cancellation
% By: Rohan Rohilla
clear
close all
clc
%% Pure LMS
% Discussion:
% LMS is very simple and stable algorithm. But its convergence rate depends
% on the eigenvalues of the autocorrelation of the input and thus make it less
% usable in practical situations (in other words LMS is sensitive to the scaling
% of its input). From the simulations it is observed that the oerder of the
% filter is inversely proportion to the convergece rate. 
% To maintain the stability of the algorithm mu has to be lower than
% 1/(3*tr[R]). 

tic
comp_times = zeros(1,7);
tim = cputime;
H = [0.35 1 -0.35]';
% H = [0.35 1 0.35]';
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
N = 15;%6,10,15 (Order of the Filter)
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.2;
mu = misad/trace(R);
in_p_samps = 400; % number of samples
var = 0.001;
realizations = 600;
w1 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
for m = 1:realizations % Number of realizations
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    for n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w1(:,n)'*X);
        e(n) = d(n) - y(n);
        w1(:,n+1) = w1(:,n) + 2*mu*e(n)*X; % Recursion Equation
    end
    SE(m,:) = e.^2; % Square Error
end
comp_times(1) = cputime - tim;
t = 1:in_p_samps; % Mean Square Error
MMSE  = var*ones(in_p_samps,1); % Minimum MSE
MSE = mean(SE);
figure;
semilogy(t,MSE,t,MMSE,'REPLACE_WITH_DASH_DASHk')
title(['Pure LMS for N = ', num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel('NO. OF ITERATIONS')
ylabel('MSE')
legend('MSE Convergence','Minimum MSE');
grid

figure;
plot(t,w1(:,1:end-1))
xlabel('NO. OF ITERATIONS')
ylabel('Coefficients')
title('Pure LMS Coefficient Convergence')
grid on
%% Sign LMS
% Discussion:
% Sign LMS was developed to reduce the computational complexity of the
% pure LMS on the cost of convergence speed and this can be noted easily
% from the Bar graph at the end of the program. Sign LMS takes around 1200
% iterations to converge to the optimal coefficients. This is because we
% can see the step-size parameter as a function of the error (mu' = mu/|e(n)|)
% Here initially step size is very small because of the huge |e(n)| but as
% the error decreases the step-size increases and this can be seen in the
% plot. For Sign LMS as well convergence rate decreasaes when we increase
% the order of the filter.

clearvars -except H w0_den w0_num w1 comp_times
H = [0.35 1 -0.35]';
% H = [0.35 1 0.35]';
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;%6,10,15 (Order of the Filter)
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.010;
mu = misad/trace(R);
in_p_samps = 2000; % number of samples
var = 0.001;
realizations = 2000;
w2 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
for m = 1:realizations % Number of realizations
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    for n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w2(:,n)'*X);
        e(n) = d(n) - y(n);
        mu_dash = mu/abs(e(n));
        w2(:,n+1) = w2(:,n) + 2*mu_dash.*(e(n))*X; % Recursion Equation
    end
    SE(m,:) = (e).^2; % Square Error
end
comp_times(2) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); % Mean Square Error
MMSE  = var*ones(in_p_samps,1); % Minimum MSE
figure;
semilogy(t,MSE,t,MMSE,'REPLACE_WITH_DASH_DASHk')
title(['Sign LMS for N = ', num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel('NO. OF ITERATIONS')
ylabel('MSE')
legend('MSE Convergence','Minimum MSE');
grid

figure;
plot(t,w2(:,1:end-1))
xlabel('NO. OF ITERATIONS')
ylabel('Coefficients')
title('Signed LMS Coefficient Convergence')
grid on
%%  Sign-Regressor LMS
% Discussion:
% Although built on the same line as sign LMS (to reduce the computational
% complexity) Sign regressor is almost as fast as pure LMS because in this
% case the new step size doesn't depend on the filter convergence. The step
% size is chosen on the basis of the average of |x(n)| and this gives a
% more homogeneous convergence. Also on a similar oberservation about the
% learning curve and the order of the filter can be made.

clearvars -except H w0_den w0_num w1 w2 comp_times
H = [0.35 1 -0.35]';
% H = [0.35 1 0.35]';
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;%6,10,15 (Order of the Filter)
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.2;
mu = 0.0107; %misad/trace(R)
in_p_samps = 400; % number of samples
var = 0.001;
realizations = 1000;
w3 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
for m = 1:realizations % Number of realizations
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    for n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w3(:,n)'*X);
        e(n) = d(n) - y(n);
        w3(:,n+1) = w3(:,n) + 2*mu*e(n)*X./abs(X); % Recursion Equation
    end
    SE(m,:) = (e).^2; % Square Error
end
comp_times(3) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); % Mean Square Error
MMSE  = var*ones(in_p_samps,1); % Minimum MSE
figure;
semilogy(t,MSE,t,MMSE,'REPLACE_WITH_DASH_DASHk')
title(['Sign Regressor LMSfor N = ', num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel('NO. OF ITERATIONS')
ylabel('MSE')
legend('MSE Convergence','Minimum MSE');
grid

figure;
plot(t,w3(:,1:end-1))
xlabel('NO. OF ITERATIONS')
ylabel('Coefficients')
title('Sign Regressor Coefficient Convergence')
grid on 
%% Signed-Signed LMS
% Discussion:
% Sign-sign LMS is nothing but the mixture of sign and sign regressor
% algorithm. Among given two algorithms it is close to sign because its step size
% parameter depends on the filter convergence and therefore shows the same
% characteristics as the sign algorithm. Because it doesn't use the values
% of the error and input it is computationally simple and can be used in
% practical scenarios where speed is not necessary for example in medical
% equipment.

clearvars -except H w0_den w0_num w1 w2 w3 comp_times
H = [0.35 1 -0.35]';
% H = [0.35 1 0.35]';
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;%6,10,15 (Order of the Filter)
HACR = xcorr(H,H);
R = toeplitz([HACR((end+1)/2:end);zeros(N-length(H),1)]);
misad = 0.2;
mu = 0.0003; %misad/trace(R) (Step Size)
in_p_samps = 4000; % number of samples
var = 0.001;
realizations = 600;
w4 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
for m = 1:realizations  % Number of realizations
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    for n = N:in_p_samps
        X = x(n:-1:n-N+1);
        y(n) = (w4(:,n)'*X);
        e(n) = d(n) - y(n);
        w4(:,n+1) = w4(:,n) + 2*mu.*sign(e(n)*X); % Recursion Equation
    end
    SE(m,:) = e.^2; % Square Error
end
comp_times(4) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); % Mean Square Error
MMSE  = var*ones(in_p_samps,1); % Minimum MSE
figure;
semilogy(t,MSE,t,MMSE,'REPLACE_WITH_DASH_DASHk')
title(['Signed-Signed LMS for N = ', num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel('NO. OF ITERATIONS')
ylabel('MSE')
legend('MSE Convergence','Minimum MSE');
grid

figure;
plot(t,w4(:,1:end-1))
xlabel('NO. OF ITERATIONS')
ylabel('Coefficients')
title('Signed Signed LMS Coefficient Convergence')
grid on
%% Normalized LMS
% Discussion:
% Major Problem with Pure LMS algorithm is that its sensitivity depends on
% the scaling of the input. Normalized LMS takes care of this problem by
% normalizing the power of the input. Here mu^ and Psi are positive constants,
% mu^ can be thought of as a step size parameter which controls the
% convergence of the algorithm and Psi is added in the denominator in
% order to prevent a division by a small number (x'x -> Euclidean Norm).
% This results in a stable and a fast converging adaptation algorithm.
% When the value for mu^ is changed the convergence behavior changes and
% by that it can be said that the mu^ is directly proportional to the
% convergence rate (if we change mu^ from 0.5 to 1 rate of convergence
% increases).

clearvars -except H w0_den w0_num w1 w2 w3 w4 comp_times
H = [0.35 1 -0.35]';
% H = [0.35 1 0.35]';
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;%6,10,15 (Order of the Filter)
mu_hat = 1;%1,0.5
sci = 0.0001;
in_p_samps = 400; % number of samples
var = 0.001;
realizations = 600;
w5 = zeros(N,in_p_samps);
e = zeros(1,in_p_samps);
SE = zeros(realizations,in_p_samps);
for m = 1:realizations  % Number of realizations
    v = randn(1,in_p_samps);
    x = filter(H',1,v)';
    d_dash = filter(w0_num',w0_den',x');
    d = d_dash + sqrt(var)*randn(1,in_p_samps);
    for n = N:in_p_samps % number of runs
        X = x(n:-1:n-N+1);
        y(n) = (w5(:,n)'*X);
        e(n) = d(n) - y(n);
        w5(:,n+1) = w5(:,n) + mu_hat*(e(n)*X)./(2*(X'*X) + sci); % Recursion Equation
    end
    SE(m,:) = e.^2; % Square Error
end
comp_times(5) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); % Mean Square Error
MMSE  = var*ones(in_p_samps,1); % Minimum MSE
figure;
semilogy(t,MSE,t,MMSE,'REPLACE_WITH_DASH_DASHk')
title(['Normalized LMS with Mu Hat = ', num2str(mu_hat),' and N = ',num2str(N)])
axis([0,in_p_samps,var*0.8,100]);
xlabel('NO. OF ITERATIONS')
ylabel('MSE')
legend('MSE Convergence','Minimum MSE');
grid

figure;
plot(t,w5(:,1:end-1))
xlabel('NO. OF ITERATIONS')
ylabel('Coefficients')
title('NLMS Coefficient Convergence')
grid on
%% Affine Projection LMS
% Discussion:
% Affine Projection is a generalized form or Normalized LMS which can also
% be called as the Orthogonal Projection LMS. Here mu^ and Psi are used for
% the same purposes as in the NLMS algorithm. From the simulations it can
% be seen that the convergence rate increases as we increase M. For M = 1 it
% behaves like Normalized LMS. Computation time of APLMS for the
% simulation is very high as well (Just like its computation complexity).

clearvars -except H w0_den w0_num w1 w2 w3 w4 w5 comp_times
H = [0.35 1 -0.35]';
% H = [0.35 1 0.35]';
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;%10,6,15 (Order of the Filter)
mu_hat = 0.5;%1
psi = 1e-4;
M = 2;%2,4,6,8 % number of colums
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
in_p_samps = 800;
var = 0.001;
realizations = 600;
for p = 1:realizations
    v = randn(1,in_p_samps)';
    x = filter(H,1,v);% Generation of input x(n)
    d = filter(w0_num,w0_den,x)+sqrt(var)*randn(in_p_samps,1);
    XAF = zeros(N,M);
    w6 = zeros(N,in_p_samps);
    for m = N+M:in_p_samps
        for k = 1:M
            XAF(:,k) = x(m-k+1:-1:m-k+1-N+1);
        end
        E = d(m:-1:m-M+1) - XAF'*w6(:,m);
        w6(:,m+1) = w6(:,m) + mu_hat*XAF*inv((XAF'*XAF + psi*eye(M)))*E;
        e(m) = E(1)'*E(1);
    end
    SE(p,:) = e';
end
comp_times(6) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); % Mean Square Error
MMSE  = var*ones(in_p_samps,1); % Minimum MSE
figure;
semilogy(t,MSE,t,MMSE,'REPLACE_WITH_DASH_DASHk')
title(['Affine Projection LMS with Mu^ = ', num2str(mu_hat),' and M = ' num2str(M)])
axis([0,in_p_samps,var*0.8,100]);
xlabel('NO. OF ITERATIONS')
ylabel('MSE')
legend('MSE Convergence','Minimum MSE');
grid

figure;
plot(t,w6(:,1:end-1))
xlabel('NO. OF ITERATIONS')
ylabel('Coefficients')
title('APLMS Coefficient Convergence')
grid on
%% Recursive Least Squares
% Discussion:
% RLS is different from the rest of the algorithms because unlike others it
% is not based on stochastic behavior of the processes. It is assumed that
% all the processes are ergodic and thus their time average is taken
% instead of the ensemble average. Thus RLS is a deterministic algorithm.
% This property of RLS makes it more usable in real situations.
% It is observed from the simulations that the computation time of RLS algorithm
% is better than APLMS and Sign LMS, and it takes the least number of iterations
% to converge (for a particular set of parameters).
% From the plots it is also verified that the fast convergence starts right
% after the N samples of input.
% This algorithm was tested for different values of delta and no change in
% the convergence rate and computation time was observed. The variable
% Lambda is forgetting factor and it governs the convergence of the
% algorithm. For a lambda as low as 0.1 the algorithm diverges and if the
% lambda value is increased the MSE decreases. One strange thing I observed
% in the simulation was that, if I increase the value of Lambda to 0.9 the
% MSE decreases, this time even lower than the minimum MSE!! 

clearvars -except H w0_den w0_num w1 w2 w3 w4 w5 w6 comp_times
H = [0.35 1 -0.35]';
% H = [0.35 1 0.35]';
w0_num = [1 0 0]';
w0_den = [1 -0.3 0.2]';
tim = cputime;
N = 15;%10,6,15 (Order of the Filter)
delta = 1e0;% 1e-3,1e-2,1e-1,1
lambda = 0.8; % Forgetting Factor
var = 0.001;
in_p_samps = 400;
sd = sqrt(0.001);
realizations = 600;
s = (1/delta)*eye(15);
w7 = zeros(15,(in_p_samps));
for n = 1:realizations % Deifferent Realizations
    v = randn(1,in_p_samps)';
    x = filter(H,1,v);% Generation of input x(n)
    d = filter(w0_num,w0_den,x) + (sd)*rand(in_p_samps,1);
    u = zeros(N,1);
    s = (1/delta)*eye(15);
    y = zeros(1,in_p_samps);
    e = zeros(1,in_p_samps);
    xaf = zeros(N,1);
    n_temp = zeros(N);
    k = zeros(1,in_p_samps);
    for m = N:in_p_samps
        xaf = x(m:-1:m-N+1);
        u = s*xaf;
        k= u./(lambda+(xaf'*u));
        y(m) = w7(:,m)'*xaf;
        e(m) = d(m) - y(m);
        w7(:,m+1) = w7(:,m) + k*e(m); % Recursion Equation
        % n_temp = (lambda^-1)*(s - k*xaf'*s); % to maintain symmetry 
        % st = triu(n_temp,1);d_s = eye(15).*diag(n_temp);
        % s = st + d_s + st'; % Making Symmetric s matrix (Psi Inverse matrix)
        s = (lambda^-1)*(s - k*xaf'*s); % Comment this 
    end
    SE(n,:) = e.^2; % Square Error
end
comp_times(7) = cputime - tim;
t = 1:in_p_samps;
MSE = mean(SE); % Mean Square Error
MMSE  = var*ones(in_p_samps,1); % Minimum MSE
figure;
semilogy(t,MSE,t,MMSE,'REPLACE_WITH_DASH_DASHk')
title(['RLS with \delta = ', num2str(delta),' and \lambda = ' num2str(lambda)])
axis([0,in_p_samps,var*0.8,100]);
xlabel('NO. OF ITERATIONS')
ylabel('MSE')
legend('MSE Convergence','Minimum MSE');
grid

figure;
plot(t,w7(:,1:end-1))
xlabel('NO. OF ITERATIONS')
ylabel('Coefficients')
title('RLS Coefficient Convergence')
grid on
%% Frequency Response Comparision
lms = w1(:,end); % Modeled System Coefficients for LMS
slms = w2(:,end); % Modeled System Coefficients for Sign LMS
srlms = w3(:,end); % Modeled System Coefficients for Sign Regressor LMS
sslms = w4(:,end); % Modeled System Coefficients for Signed-Signed LMS
nlms = w5(:,end); % Modeled System Coefficients for Normalized LMS
aplms = w6(:,end); % Modeled System Coefficients for Affine Projection LMS
rls = w7(:,end); % Modeled System Coefficients for Recursive Lear Squares

[LMS,wlms]=freqz(lms,1);
[SLMS,wslms]=freqz(slms,1);
[SRLMS,wsrlms]=freqz(srlms,1);
[SSLMS,wsslms]=freqz(sslms,1);
[NLMS,wnlms]=freqz(nlms,1);
[APLMS,waplms]=freqz(aplms,1);
[RLS,wrls]=freqz(rls,1);

[OR_FILT,wor_filt] = freqz(w0_num,w0_den); % Frequency Response of the Original IIR filter

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wlms/(2*pi),20*log10(abs(LMS)),'linewidth',2)
legend('Original IIR Filter','Regular LMS')
ylabel('Log_magnitude');
xlabel('Normalized Frequency');
grid on

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wslms/(2*pi),20*log10(abs(SLMS)),'linewidth',2)
legend('Original IIR Filter','Sign LMS')
ylabel('Log_magnitude');
xlabel('Normalized Frequency');
title('Frequency Response Comparision Between the Original IIR filter and the Identified Filter')
grid on

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wsrlms/(2*pi),20*log10(abs(SRLMS)),'linewidth',2)
legend('Original IIR Filter','Sign Regressor LMS')
ylabel('Log_magnitude');
xlabel('Normalized Frequency');
title('Frequency Response Comparision Between the Original IIR filter and the Identified Filter')
grid on

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wsslms/(2*pi),20*log10(abs(SSLMS)),'linewidth',2)
legend('Original IIR Filter','Sign Sign LMS')
ylabel('Log_magnitude');
xlabel('Normalized Frequency');
title('Frequency Response Comparision Between the Original IIR filter and the Identified Filter')
grid on

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wnlms/(2*pi),20*log10(abs(NLMS)),'linewidth',2)
legend('Original IIR Filter','Normalized LMS')
ylabel('Log_magnitude');
xlabel('Normalized Frequency');
title('Frequency Response Comparision Between the Original IIR filter and the Identified Filter')
grid on

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),waplms/(2*pi),20*log10(abs(APLMS)),'linewidth',2)
legend('Original IIR Filter','Affine Projection LMS')
ylabel('Log_magnitude');
xlabel('Normalized Frequency');
title('Frequency Response Comparision Between the Original IIR filter and the Identified Filter')
grid on

figure;
plot(wor_filt/(2*pi),20*log10(abs(OR_FILT)),wrls/(2*pi),20*log10(abs(RLS)),'linewidth',2)
legend('Original IIR Filter','Regular LMS')
ylabel('Log_magnitude');
xlabel('Normalized Frequency');
title('Frequency Response Comparision Between the Original IIR filter and the Identified Filter')
grid on
%% Computation Time Comparision
bar_name = {'LMS','SLMS','SRLMS','SSLMS','NLMS','APLMS','RLS'};
figure;
bar(comp_times)
title('Convergence Time for each Algorithm (in s)')
ylabel('Time in Seconds')
xlabel('Algorithms')
grid
set(gca,'xticklabel', bar_name)
toc
%% Comparision and Discussion of all the Algorithms 

% At the end we can pick a winner in terms of computation time, computation
% complexity, number of iterations required (and also on number of
% realization required, but in practical situations we only have one
% realization so there is not point of taking number of realizations as a
% performance metric). In case of computation complexity it is well known
% that the sign-sign LMS beats all of these algorithms (from the textbook
% Adaptive Filters theory and application). But on the other hand it takes
% more iterations than the rest of the algorithms to converge.
 
% Similar to sign sign lms, sign lms and sign regressor also takes more
% than reasonable iterations. In their defense there job was not to reduce
% the number of iteration but to reduce the computation complexity.
 
% For faster convergence I would pick NLMS over any of the give algorithms
% because it took significantly less time and iterations. Although Pure LMS
% beats NLMS when it comes to computation time but it takes more iterations
% than NLMS as well and the difference in timings is not that big. Again
% there is no free lunch! NLMS is beautiful but it based on statistical
% parameters and thus requires more realizations than 1 (and thats all what
% we have got in reality). This is where RLS comes into play. It is purely
% deterministic and speed is comparable as well. With this facility comes
% the cost and that is more computations (RLS requires 4N^2 multiplications
% and 3N^2 additions).
 
% Frequency responses of the identified filters (from each algorithm) are
% also plotted and compared with the original IIR filter. All of them
% matches very well, except that a few have ripple in the stop band and
% in the pass band. As the Filters are just the approximation to the
% original filter this kind of side effect is bound to come across.
##### SOURCE END #####
--></body></html>